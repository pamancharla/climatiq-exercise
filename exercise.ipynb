{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two files attached containing the following data:\n",
    "\n",
    "- input-data.csv: details of 50 items from some user input.\n",
    "- activities.csv: a list of activities representing emission factors. Each activity is composed by a name (“activity_name” column), and is grouped by a set of categories (“category” column).\n",
    "\n",
    "Note: All additional information has been striped for simplicity, but for sake of completeness: these activities contain metadata about their respective emissions.\n",
    "\n",
    "Task: You are tasked with developing a system that finds the most relevant match for each of the input data strings from input-data.csv to activity names in activities.csv.\n",
    "\n",
    "Deliverable: Submit your solution as a Python script or a Jupyter notebook, including the following requirements:\n",
    "- Build a prototype model, including data processing, labeling and evaluation.\n",
    "- For each unstructured input record, return the best-matched activity (name and category) along with a score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_data_df = pd.read_csv(\"input-data.csv\")\n",
    "activities_df = pd.read_csv(\"activities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Shape: (49, 1)\n",
      "Activites Shape: (7133, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input Data Shape: {input_data_df.shape}\")\n",
    "print(f\"Activites Shape: {activities_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df = activities_df.drop_duplicates(subset=[\"activity_name\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activites Shape: (7133, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Activites Shape: {activities_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drill bits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Light bulb filaments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PACKAGING PALLET S7A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steel alloys for rails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hex bolt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Catalysts in chemical reactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t-joint, steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Aircraft structures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Thermometers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Airplane fuselages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              item\n",
       "0                       Drill bits\n",
       "1             Light bulb filaments\n",
       "2             PACKAGING PALLET S7A\n",
       "3           Steel alloys for rails\n",
       "4                         hex bolt\n",
       "5  Catalysts in chemical reactions\n",
       "6                   t-joint, steel\n",
       "7              Aircraft structures\n",
       "8                     Thermometers\n",
       "9               Airplane fuselages"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gas condensing boiler 120-400kW (upright unit)</td>\n",
       "      <td>Electrical Equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spinach (market for spinach)</td>\n",
       "      <td>Arable Farming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potatoes crisps</td>\n",
       "      <td>Food/Beverages/Tobacco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tax preparation services</td>\n",
       "      <td>Professional Services and Activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aluminium - North American</td>\n",
       "      <td>Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Savings institutions</td>\n",
       "      <td>Financial Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hydrogen gaseous low pressure (market for hydr...</td>\n",
       "      <td>Fuel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Other commercial buildings</td>\n",
       "      <td>Real Estate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Aircraft</td>\n",
       "      <td>Vehicles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>All other insurance related activities</td>\n",
       "      <td>Insurance Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       activity_name  \\\n",
       "0     Gas condensing boiler 120-400kW (upright unit)   \n",
       "1                       Spinach (market for spinach)   \n",
       "2                                    Potatoes crisps   \n",
       "3                           Tax preparation services   \n",
       "4                         Aluminium - North American   \n",
       "5                               Savings institutions   \n",
       "6  Hydrogen gaseous low pressure (market for hydr...   \n",
       "7                         Other commercial buildings   \n",
       "8                                           Aircraft   \n",
       "9             All other insurance related activities   \n",
       "\n",
       "                               category  \n",
       "0                  Electrical Equipment  \n",
       "1                        Arable Farming  \n",
       "2                Food/Beverages/Tobacco  \n",
       "3  Professional Services and Activities  \n",
       "4                                Metals  \n",
       "5                    Financial Services  \n",
       "6                                  Fuel  \n",
       "7                           Real Estate  \n",
       "8                              Vehicles  \n",
       "9                    Insurance Services  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 categories\n"
     ]
    }
   ],
   "source": [
    "num_categories = len(activities_df[\"category\"].unique())\n",
    "print(f\"{num_categories} categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max category length: 820, Min category length: 1\n",
      "Median category length: 34.0\n",
      "Mean category length: 103.3768115942029\n"
     ]
    }
   ],
   "source": [
    "category_to_activity = {}\n",
    "category_lengths = []\n",
    "for category in activities_df[\"category\"].unique():\n",
    "    category_to_activity[category] = activities_df.index[activities_df[\"category\"] == category].to_list()\n",
    "    category_lengths.append(len(category_to_activity[category]))\n",
    "\n",
    "print(f\"Max category length: {max(category_lengths)}, Min category length: {min(category_lengths)}\")\n",
    "print(f\"Median category length: {pd.Series(category_lengths).median()}\")\n",
    "print(f\"Mean category length: {pd.Series(category_lengths).mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 Aluminium - not alloyed - bars / rods\n",
      "471 Iron non-alloy steel - bars / rods / long\n",
      "848 Steel - Wire rod\n",
      "1424 Iron non-alloy steel - bars rods hot-rolled drawn extruded forgings\n",
      "1520 Steel - hollow drill bars rods - forgings\n",
      "1864 Iron non-alloy steel - bars rods hot-rolled coils\n",
      "2311 Stainless steel - shapes sections bars rods long\n",
      "2761 Iron non-alloy steel - bars rods nec\n",
      "3845 Aluminium - alloys - bars / rods\n",
      "4140 Steel alloy - bars rods hot-rolled coils\n",
      "5459 Stainless steel - bars rods hot-rolled coils\n",
      "6026 Iron non-alloy steel - hot-rolled drawn extruded - bars rods long\n",
      "6084 Steel - hollow bars rods long\n"
     ]
    }
   ],
   "source": [
    "category = None\n",
    "activity_target = [\" rod\"]\n",
    "for index, row in activities_df.iterrows():\n",
    "    found_activity = True\n",
    "    if activity_target is not None:\n",
    "        found_activity = False\n",
    "        for t in activity_target:\n",
    "            if t in row[\"activity_name\"].lower():\n",
    "                found_activity = True\n",
    "                break\n",
    "    if found_activity and (category is None or row[\"category\"] in category):\n",
    "        print(index, row[\"activity_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activity_name             Light bulbs\n",
       "category         Electrical Equipment\n",
       "Name: 6671, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities_df.iloc[6671]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following is in the format of:\n",
    "\"input item\": [(activity, score), (activity, score), ...]\n",
    "\n",
    "Items for which ground truth rankings were obtained:\n",
    "\"Control rods in nuclear ractors\": []\n",
    "\"\"\"\n",
    "\n",
    "test_set_relevant_mappings = {\n",
    "    13: [2930, 471, 2311],\n",
    "    27: [349, 770, 3745],\n",
    "    14: [3213, 4975],\n",
    "    45: [4277, 5433, 993],\n",
    "    38: [103, 1052],\n",
    "    10: [6946, 2635, 2307, 5035, 5537],\n",
    "    36: [25, 3044, 5329, 5484, 6765],\n",
    "    1: [3501, 6671],\n",
    "    42: [467],\n",
    "    24: [5807]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def average_precision_at_k(predicted_indices: List[int], relevant_indices: List[int], k: int) -> float:\n",
    "    num_relevant = 0\n",
    "    sum_precision_at_k = 0\n",
    "    for i, index in enumerate(predicted_indices):\n",
    "        if index in relevant_indices:\n",
    "            num_relevant += 1\n",
    "            sum_precision_at_k += num_relevant / (i + 1) # add precision at k\n",
    "        if i + 1 >= k:\n",
    "            break\n",
    "    return sum_precision_at_k / min(k, len(relevant_indices))\n",
    "\n",
    "\n",
    "def mean_average_precision_at_k(\n",
    "        item_to_predicted_indices: Dict[int, List[List[int]]],\n",
    "        test_set_relevant_mappings: Dict[int, List[List[int]]],\n",
    "        k: int\n",
    ") -> float:\n",
    "    sum_average_precision_at_k = 0\n",
    "    for item_index, predicted_indices in item_to_predicted_indices.items():\n",
    "        sum_average_precision_at_k += average_precision_at_k(predicted_indices, test_set_relevant_mappings[item_index], k)\n",
    "    return sum_average_precision_at_k / len(item_to_predicted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_closest_matches(query_embedding: List[float], target_embeddings: List[List[float]], k: int = 5) -> List[int]:\n",
    "    similarities = cosine_similarity([query_embedding], target_embeddings)\n",
    "    return np.argsort(similarities)[0][-5:][::-1]\n",
    "\n",
    "\n",
    "def get_test_set_performance(\n",
    "        query_embeddings: List[List[float]],\n",
    "        target_embeddings: List[List[float]],\n",
    "        test_set_relevant_mappings: Dict[int, List[int]],\n",
    "        k: List[int]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Given the predicted query and target embeddings, return the mean average precision at all k values\n",
    "    provided.\n",
    "    \"\"\"\n",
    "    item_to_predicted_activity_indices = {}\n",
    "    for test_index in test_set_relevant_mappings:\n",
    "        test_embedding = query_embeddings[test_index]\n",
    "        closest_matches = get_closest_matches(test_embedding, target_embeddings)\n",
    "        item_to_predicted_activity_indices[test_index] = closest_matches\n",
    "\n",
    "    map_at_k = []\n",
    "    for k_val in k:\n",
    "        map_at_k.append(mean_average_precision_at_k(item_to_predicted_activity_indices, test_set_relevant_mappings, k_val))\n",
    "    return map_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df[\"combined_string\"] = activities_df.apply(lambda x: x[\"category\"] + \": \" + x[\"activity_name\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@1: 0.3, MAP@5: 0.22333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert text to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words=\"english\") # Use bigrams on top of single words\n",
    "query_embeddings = vectorizer.fit_transform(input_data_df[\"item\"].tolist()).toarray()\n",
    "target_embeddings = vectorizer.transform(activities_df[\"combined_string\"].tolist())\n",
    "map_at_k = get_test_set_performance(query_embeddings, target_embeddings, test_set_relevant_mappings, [1, 5])\n",
    "print(f\"MAP@1: {map_at_k[0]}, MAP@5: {map_at_k[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Sentence Transformer Model Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preetamamancharla/.pyenv/versions/3.11.5/envs/climatiq-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@1: 0.4, MAP@5: 0.29833333333333334\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "mini_lm_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "target_embeddings = mini_lm_model.encode(activities_df[\"combined_string\"].tolist())\n",
    "query_embeddings = mini_lm_model.encode(input_data_df[\"item\"].tolist())\n",
    "\n",
    "map_at_k = get_test_set_performance(query_embeddings, target_embeddings, test_set_relevant_mappings, [1, 5])\n",
    "print(f\"MAP@1: {map_at_k[0]}, MAP@5: {map_at_k[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intfloat/e5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@1: 0.5, MAP@5: 0.37416666666666665\n"
     ]
    }
   ],
   "source": [
    "infloat_model = SentenceTransformer(\"intfloat/e5-base\")\n",
    "target_embeddings = infloat_model.encode(activities_df[\"combined_string\"].tolist())\n",
    "query_embeddings = infloat_model.encode(input_data_df[\"item\"].tolist())\n",
    "\n",
    "map_at_k = get_test_set_performance(query_embeddings, target_embeddings, test_set_relevant_mappings, [1, 5])\n",
    "print(f\"MAP@1: {map_at_k[0]}, MAP@5: {map_at_k[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@1: 0.4, MAP@5: 0.2833333333333333\n"
     ]
    }
   ],
   "source": [
    "mpnet_base_model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
    "target_embeddings = mpnet_base_model.encode(activities_df[\"combined_string\"].tolist())\n",
    "query_embeddings = mpnet_base_model.encode(input_data_df[\"item\"].tolist())\n",
    "\n",
    "map_at_k = get_test_set_performance(query_embeddings, target_embeddings, test_set_relevant_mappings, [1, 5])\n",
    "print(f\"MAP@1: {map_at_k[0]}, MAP@5: {map_at_k[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use intfloat/e5-base for final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "infloat_model = SentenceTransformer(\"intfloat/e5-base\")\n",
    "target_embeddings = infloat_model.encode(activities_df[\"combined_string\"].tolist())\n",
    "query_embeddings = infloat_model.encode(input_data_df[\"item\"].tolist())\n",
    "\n",
    "output_activity = []\n",
    "output_category = []\n",
    "output_scores = []\n",
    "for query_embedding in query_embeddings:\n",
    "    similarities = cosine_similarity([query_embedding], target_embeddings)\n",
    "    best_match_index = np.argmax(similarities)\n",
    "    output_activity.append(activities_df.iloc[best_match_index][\"activity_name\"])\n",
    "    output_category.append(activities_df.iloc[best_match_index][\"category\"])\n",
    "    output_scores.append(similarities[0][best_match_index])\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    \"item\": input_data_df[\"item\"],\n",
    "    \"activity\": output_activity,\n",
    "    \"category\": output_category,\n",
    "    \"score\": output_scores\n",
    "})\n",
    "output_df.to_csv(\"output-data.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning intfloat/e5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"intfloat/e5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data = [\n",
    "    {\"query\": \"query: example query 1\", \"positive\": \"similar to example query 1\", \"negative\": \"not similar to example query 1\"},\n",
    "    {\"query\": \"query: example query 2\", \"positive\": \"similar to example query 2\", \"negative\": \"not similar to example query 2\"},\n",
    "    {\"query\": \"query: similar to example query 3\", \"positive\": \"similar to example query 3\", \"negative\": \"not similar to example query 3\"},\n",
    "]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data) # huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 724.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_triplets(examples):\n",
    "    anchor = tokenizer(examples[\"query\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    positive = tokenizer(examples[\"positive\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    negative = tokenizer(examples[\"negative\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    return {\n",
    "        \"input_ids_query\": anchor[\"input_ids\"],\n",
    "        \"attention_mask_query\": anchor[\"attention_mask\"],\n",
    "        \"input_ids_positive\": positive[\"input_ids\"],\n",
    "        \"attention_mask_positive\": positive[\"attention_mask\"],\n",
    "        \"input_ids_negative\": negative[\"input_ids\"],\n",
    "        \"attention_mask_negative\": negative[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(tokenize_triplets, batched=True, remove_columns=[\"query\", \"positive\", \"negative\"])\n",
    "train_dataset.set_format(type=\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TripletLossModel(nn.Module):\n",
    "    def __init__(self, model, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.TripletMarginLoss(margin=margin, p=2)  # Euclidean distance\n",
    "\n",
    "    def forward(self, input_ids_anchor, attention_mask_anchor, \n",
    "                      input_ids_positive, attention_mask_positive, \n",
    "                      input_ids_negative, attention_mask_negative):\n",
    "        \n",
    "        anchor_emb = self.model(input_ids_anchor, attention_mask=attention_mask_anchor).last_hidden_state[:, 0, :]\n",
    "        positive_emb = self.model(input_ids_positive, attention_mask=attention_mask_positive).last_hidden_state[:, 0, :]\n",
    "        negative_emb = self.model(input_ids_negative, attention_mask=attention_mask_negative).last_hidden_state[:, 0, :]\n",
    "\n",
    "        loss = self.loss_fn(anchor_emb, positive_emb, negative_emb)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preetamamancharla/.pyenv/versions/3.11.5/envs/climatiq-env/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 17\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./e5_finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m loss_model \u001b[38;5;241m=\u001b[39m MultipleNegativesRankingLoss(model)\n\u001b[0;32m---> 17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# trainer.train()\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/climatiq-env/lib/python3.11/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/climatiq-env/lib/python3.11/site-packages/transformers/trainer.py:459\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/climatiq-env/lib/python3.11/site-packages/transformers/trainer.py:5068\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5065\u001b[0m     args\u001b[38;5;241m.\u001b[39mupdate(accelerator_config)\n\u001b[1;32m   5067\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 5068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5069\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   5070\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/climatiq-env/lib/python3.11/site-packages/accelerate/accelerator.py:495\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusa\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(\n\u001b[1;32m    493\u001b[0m     check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    494\u001b[0m ):\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    496\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m get_grad_scaler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./e5_finetuned\",\n",
    "    per_device_train_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "model = TripletLossModel(model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climatiq-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
